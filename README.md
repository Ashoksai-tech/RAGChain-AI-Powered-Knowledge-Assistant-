# 🔍 Chat with Documents using LangChain, FAISS, and GROQ

This is a simple and powerful Retrieval-Augmented Generation (RAG) application built with [LangChain](https://www.langchain.com/), [FAISS](https://github.com/facebookresearch/faiss), [GROQ](https://groq.com/), and [Streamlit](https://streamlit.io/). It allows you to query content from web documents using advanced LLM capabilities.

## 🚀 Features

- 🌐 Load documents from a website using `WebBaseLoader`
- 📚 Split documents into chunks for efficient retrieval
- 🧠 Create embeddings using `Ollama` (nomic-embed-text)
- 🔎 Store and retrieve embeddings using `FAISS`
- 🤖 Query responses generated by `GROQ LLM` (e.g., `llama3-8b-8192`)
- 💬 Interactive Streamlit UI for asking questions
- 📄 Expandable document view showing sources used in the answer

---

## 📦 Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/your-username/your-repo-name.git
   cd your-repo-name
   ```

2. **Create and activate a virtual environment:**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows use `venv\Scripts\activate`
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Set up your `.env` file:**
   ```
   GROQ_API_KEY=your_groq_api_key_here
   ```

5. **Pull the required Ollama model:**
   ```bash
   ollama pull nomic-embed-text
   ```

---

## ▶️ Run the App

```bash
streamlit run agent.py
```

---

## 📝 Notes

- Make sure your [Ollama](https://ollama.com/) server is running locally.
- The application currently loads and queries content from [LangChain Python Docs](https://python.langchain.com/).

---
